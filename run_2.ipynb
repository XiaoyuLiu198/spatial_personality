{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import statistics\n",
    "from itertools import chain\n",
    "import copy\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from os.path import dirname\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "import os\n",
    "import ipywidgets\n",
    "\n",
    "# import fastllm_pytools\n",
    "# from fastllm_pytools import llm\n",
    "\n",
    "# To run this, cmd: python single.py --file xx --checkpoint xx --destination xx\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--file', type=str, required=True)\n",
    "# parser.add_argument('--checkpoint', type=str, required=True)\n",
    "# parser.add_argument('--destination', type=str, required=True)\n",
    "# parser.add_argument('--start', type=str, required=True)\n",
    "# parser.add_argument('--end', type=str, required=True)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# FILE = args.file\n",
    "# CKPT = args.checkpoint\n",
    "# DEST = args.destination\n",
    "# START = int(args.start)\n",
    "# END = int(args.end)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "FILE = \"twitter_prompts\"\n",
    "CKPT = \"ckpts/extraversion\"\n",
    "DEST = \"results\"\n",
    "START = 1199\n",
    "END = 2000\n",
    "\n",
    "# f = is.StringIO()\n",
    "# with redirect_stdout(f):\n",
    "#     AutoModel.from_pretrained()\n",
    "\n",
    "def pred(ds, trait):\n",
    "    # load testing set\n",
    "    dataset = pd.read_csv(ds)\n",
    "    prediction_result = []\n",
    "    instruction = f\"Predict {trait} score in big five personality based on the text. The score is integer ranging from 0 to 10.\"\n",
    "    for i in range(len(dataset)):\n",
    "        row = dataset.iloc[i, :]\n",
    "        response, history = model.chat(tokenizer, instruction + \" : \" + row[\"text\"], history=[])\n",
    "        prediction_result.append(response)\n",
    "\n",
    "    score_res = []\n",
    "    for i, label in enumerate(prediction_result):\n",
    "        number = re.findall(\"\\d+\\.\\d+\", label)\n",
    "        if len(number) == 0:\n",
    "            score_res.append(0)\n",
    "        else:\n",
    "            score_res.append(float(number[0]))\n",
    "\n",
    "    return score_res\n",
    "\n",
    "def grouping(dataset, prediction, trait):\n",
    "    ds = copy.deepcopy(dataset)\n",
    "    ds[trait + \"_prediction\"] = prediction\n",
    "    prediction_col = trait + \"_prediction\"\n",
    "    grouped = ds.groupby([\"authorid\"]).agg({ prediction_col: lambda x: list(x), 'county_fip': \"first\"})\n",
    "    votes = []\n",
    "    for i, author in enumerate(list(grouped.index)):\n",
    "    y_cnt, n_cnt = 0, 0\n",
    "    scores = []\n",
    "    for val in grouped[prediction_col][author]:\n",
    "        scores.append(val)\n",
    "        if val >= 5:\n",
    "            y_cnt += 1\n",
    "        else:\n",
    "            n_cnt += 1\n",
    "    if y_cnt < n_cnt:\n",
    "        votes.append(-1)\n",
    "    else:\n",
    "        votes.append(1)\n",
    "    grouped[trait + \"_2cls\"] = votes\n",
    "    # res = pd.merge(grouped, ds, on='authorid', how='left')\n",
    "    return grouped\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{dirname(__file__)}/'+CKPT, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(CKPT, trust_remote_code=True).half().cuda()\n",
    "# model = llm.from_hf(model_former, tokenizer, dtype = \"float16\")\n",
    "print(\"model loading completed -----------------------------\")\n",
    "\n",
    "trait = CKPT.split(\"/\")[-1]\n",
    "for i in range(START, END):\n",
    "    ds = pd.read_csv(FILE + \"batched_sample_\" + str(i) + \".csv\")\n",
    "    # res = copy.deepcopy(ds)\n",
    "\n",
    "    preds = pred(FILE + \"batched_sample_\" + str(i) + \".csv\", trait)\n",
    "    trait_res = grouping(ds, preds, trait)\n",
    "    # res = pd.merge(res, trait_res, on=\"authorid\", how=\"outer\")\n",
    "    trait_res.to_csv(DEST + trait + \"_\" + str(i) + \".csv\")\n",
    "    print(f\"completed inference for {i}-th item -----------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f65996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d11f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
